{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"position:relative; text-align:center\">Backpropagation algorithm</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://th.bing.com/th/id/OIP.GOmyJGzqoxcPYnEpXoU_kAHaD2?rs=1&pid=ImgDetMain\" style=\"display: block;\n",
    "margin-left: auto;\n",
    "margin-right: auto;\n",
    "width: 50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**abstraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MLP, or Multilayer Perceptron, is a type of artificial neural network known for its capability to classify data that is not linearly separable1. It’s a supervised learning algorithm that learns a function $$f(⋅):R_m→R_o$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m$ is the number of dimensions for input layer and $o$ is the number of dimensions for output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key characteristics of an MLP include:\n",
    "\n",
    "- **Fully Connected Layers:** Every neuron in one layer connects with a certain weight to every neuron in the following layer.\n",
    "\n",
    "- **Activation Function:** Nonlinear functions that help the network learn complex patterns.\n",
    "\n",
    "- **Backpropagation:** The method used for training the network, involving forward propagation of inputs and backward propagation of errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Backpropagation ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation is a method used to calculate the gradient of the loss function with respect to the weights of the network. This gradient is used to adjust or update the network's weights depending on its influence on the network's total prediction error. If we can continuously reduce the prediction error of each weight, we will eventually obtain a set of weights that can make sufficiently good predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ f(x) = A(B(C(x))) ⟶ f'(x) = f'(A)A'(B)B'(C)C'(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex:\n",
    "\n",
    "- $A = 2z+1 ⟶ A'(z) = 2$\n",
    "- $z = 3x+3 ⟶ z'(x) = 3$\n",
    "\n",
    "then $ A'(x) = A'(z).z'(x) = 2.3 = 6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/PhuongNam2k4/Study/assets/156770604/9ec4617f-161a-4972-8f5e-c430cf71dbfb\" style=\"display: block;\n",
    "margin-left: auto;\n",
    "margin-right: auto;\n",
    "width: 50%;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# gradient\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# initialze neural network\n",
    "def initialize_network(input_size, hidden_size, output_size):\n",
    "    np.random.seed(1)\n",
    "    hidden_weights = np.random.uniform(size=(input_size, hidden_size))\n",
    "    hidden_bias = np.random.uniform(size=(1, hidden_size))\n",
    "    output_weights = np.random.uniform(size=(hidden_size, output_size))\n",
    "    output_bias = np.random.uniform(size=(1, output_size))\n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(inputs, hidden_weights, hidden_bias, output_weights, output_bias):\n",
    "    hidden_layer_activation = np.dot(inputs, hidden_weights) + hidden_bias\n",
    "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "    \n",
    "    output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias\n",
    "    predicted_output = sigmoid(output_layer_activation)\n",
    "    \n",
    "    return hidden_layer_output, predicted_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagate(inputs, hidden_layer_output, predicted_output, actual_output, hidden_weights, hidden_bias, output_weights, output_bias, learning_rate):\n",
    "    error = actual_output - predicted_output\n",
    "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "    \n",
    "    error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "    \n",
    "    # update weight and bias\n",
    "    output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "    output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "    hidden_weights += inputs.T.dot(d_hidden_layer) * learning_rate\n",
    "    hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, actual_output, input_size, hidden_size, output_size, epochs, learning_rate):\n",
    "    hidden_weights, hidden_bias, output_weights, output_bias = initialize_network(input_size, hidden_size, output_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer_output, predicted_output = forward_propagate(inputs, hidden_weights, hidden_bias, output_weights, output_bias)\n",
    "        hidden_weights, hidden_bias, output_weights, output_bias = backpropagate(\n",
    "            inputs, hidden_layer_output, predicted_output, actual_output, hidden_weights, hidden_bias, output_weights, output_bias, learning_rate)\n",
    "        \n",
    "        if epoch % 1000 == 0:\n",
    "            loss = np.mean(np.square(actual_output - predicted_output))\n",
    "            print(f'Epoch {epoch} loss: {loss}')\n",
    "    \n",
    "    return hidden_weights, hidden_bias, output_weights, output_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.28014363590911784\n",
      "Epoch 1000 loss: 0.24971731456941582\n",
      "Epoch 2000 loss: 0.24834256246704567\n",
      "Epoch 3000 loss: 0.23517320796329574\n",
      "Epoch 4000 loss: 0.18761745745277092\n",
      "Epoch 5000 loss: 0.11376191085651667\n",
      "Epoch 6000 loss: 0.028482725248000107\n",
      "Epoch 7000 loss: 0.012042153641357224\n",
      "Epoch 8000 loss: 0.007162910776260677\n",
      "Epoch 9000 loss: 0.0049834071198557265\n",
      "Predicted Output: \n",
      " [[0.06367371]\n",
      " [0.94086271]\n",
      " [0.94109457]\n",
      " [0.06401166]]\n"
     ]
    }
   ],
   "source": [
    "# training data\n",
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "actual_output = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# initialize neural\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "epochs = 10000\n",
    "learning_rate = 0.1\n",
    "\n",
    "# train\n",
    "hidden_weights, hidden_bias, output_weights, output_bias = train(inputs, actual_output, input_size, hidden_size, output_size, epochs, learning_rate)\n",
    "\n",
    "# evaluate\n",
    "hidden_layer_output, predicted_output = forward_propagate(inputs, hidden_weights, hidden_bias, output_weights, output_bias)\n",
    "print(\"Predicted Output: \\n\", predicted_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
