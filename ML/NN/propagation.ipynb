{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xét một mạng neuron với $l$ layer sao cho mỗi một node ở một layer chỉ nhận một đầu vào từ layer trước đó. Ta gọi input layer là layer thứ 0, layer này chứa các điểm dữ liệu $x_1, x_2,...x_n$\n",
    "\n",
    "Từ layer $1$ trở đi, ta gọi là hidden layer.\n",
    "\n",
    "Ở hidden layer thứ $1$ : $$z^{(1)}_j = w^{(1)}_{ij} x + b^{(1)}_j$$\n",
    "$$ a^{(1)}_j = f(z^{(1)}_j ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở hidden layer thứ $2$ :\n",
    "\n",
    "$$z^{(2)}_j = w^{(2)}_{ij} a^{(1)}_j + b^{(2)}_j$$\n",
    "$$ a^{(2)}_j = f(z^{(2)}_j ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở hidden layer thứ $3$ :\n",
    "\n",
    "$$z^{(3)}_j = w^{(3)}_{ij} a^{(2)}_j + b^{(3)}_j$$\n",
    "$$ a^{(3)}_j = f(z^{(3)}_j ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ở hidden layer thứ $l$ :\n",
    "\n",
    "$$z^{(l)}_j = w^{(l)}_{ij} a^{(l-1)}_j + b^{(l)}_j$$\n",
    "$$ a^{(l)}_j = f(z^{(l)}_j ) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Một cách tổng quát, ta thấy rằng một node ở layer sau luôn học từ thông tin lấy từ các node ở layer trước. Khi đó, activation function cho thuật toán MLP có dạng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$ a^{(l)}_j= f(z^{(l)}_j) = f(w^{(l)}_{ij} a^{(l−1)} + b^{(l)}_j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mở rộng ra , nếu mỗi node ở một layer nhận d đầu vào (tương ứng với d node) từ layer trước thì điều gì sẽ xảy ra ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi đó:\n",
    "\n",
    "$$z^{(l)}_j = ∑_{i = 1}^d w^{(l)}_{ij} a^{(l-1)}_j + b^{(l)}_j$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://github.com/PhuongNam2k4/Study/assets/156770604/caf3bf5e-de44-4bd5-bd74-51c6330f053d\" style=\"heigh:450px; width:450px; position:relative; margin-top:50px; left: 180px; margin-bottom:50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi activation function $f()$ được áp dụng cho một ma trận (hoặc vector), ta hiểu rằng nó được áp dụng cho từng thành phần của ma trận đó. Sau đó các thành phần này được sắp xếp lại đúng theo thứ tự để được một ma trận có kích thước bằng với ma trận input. Trong tiếng Anh, việc áp dụng lên từng phần tử như thế này được gọi là element-wise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm sgn không được sử dụng trong MLP**\n",
    "\n",
    "Hàm sgn chỉ được sử dụng trong PLA, mang mục đích giáo dục nhiều hơn. Trong thực tế, hàm sgn không được sử dụng vì hai lý do: đầu ra là discrete, và đạo hàm tại hầu hết các điểm bằng 0 (trừ điểm 0 không có đạo hàm). Việc đạo hàm bằng 0 này khiến cho các thuật toán gradient-based (ví dụ như Gradient Descent) không hoạt động"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid và tanh ít khi được sử dụng** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm sigmoid có dạng \n",
    "$$f(s)= \\frac{1}{1+e^{(−z)}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://machinelearningcoban.com/assets/14_mlp/sigmoid.jpeg\" style=\"position:relative; left:300px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nếu đầu vào lớn, hàm số sẽ cho đầu ra gần với 1. Với đầu vào nhỏ (rất âm), hàm số sẽ cho đầu ra gần với 0. Hàm số này được sử dụng nhiều trong quá khứ ví có đạo hàm rất đẹp. \n",
    "\n",
    "Những năm gần đây, hàm số này ít khi được sử dụng vì một nhược điểm cơ bản:\n",
    "\n",
    "- Sigmoid saturate and kill gradients: Khi đầu vào có trị tuyệt đối lớn (rất âm hoặc rất dương), gradient của hàm số này - tức là đạo hàm của loss function sẽ rất gần với 0. Điều này đồng nghĩa với việc các trọng số tương ứng với unit đang xét sẽ gần như không được cập nhật."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm tanh cũng có nhược điểm tương tự về việc gradient rất nhỏ với các đầu vào có trị tuyệt đối lớn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm ReLU được ứng dụng nhiều hơn**\n",
    "\n",
    "ReLU (Rectified Linear Unit) được sử dụng rộng rãi gần đây vì tính đơn giản của nó. \n",
    "\n",
    "Công thức toán học: $$f(z)=max(0,z)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đồ thị:\n",
    "\n",
    "\n",
    "<img src=\"https://machinelearningcoban.com/assets/14_mlp/relu.jpeg\" style=\"position:relative; left:300px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU được chứng minh giúp cho việc training các Deep Networks nhanh hơn rất nhiều"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU được tính toán gần như tức thời và gradient của nó bằng 1 nếu đầu vào lớn hơn 0, bằng 0 nếu đầu vào nhỏ hơn 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặc dù hàm ReLU không có đạo hàm tại $z=0$, trong thực nghiệm, người ta vẫn thường định nghĩa ReLU′(0)=0 và khẳng định thêm rằng, xác suất để input của một unit bằng 0 là rất nhỏ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lan truyền xuôi - Forwardpropagation\n",
    "\n",
    "Nói một cách đơn giản, lan truyền xuôi là tất cả những gì chúng ta đã nghiên cứu ở trên, bao gồm khái niệm, công thức, lý thuyết,vv... Chúng được tổng hợp lại tạo thành một bước xây dựng thuật toán MLP được gọi là lan truyền xuôi. \n",
    "\n",
    "Cụ thể, người ta phát biểu như sau:\n",
    "\n",
    "- Lan truyền xuôi (Feedforward) là quá trình mà thông tin được truyền từ input layer qua các hiddent layer (nếu có) và cuối cùng đến output layer của mạng nơ-ron. Trong quá trình này, mỗi node ở một layer nhận đầu vào từ các layer phía trước, thực hiện một phép tính nhất định (chính là hàm tuyến tính $z$ và bias), rồi sau đó áp dụng một hàm kích hoạt để tạo ra đầu ra cho node tiếp theo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ví dụ 1:** Với các input $x_1, x_2,...,x_d$ ở layer 0, một node $z_j$ ở layer 1 sẽ được tính là tính $z_j^{(1)} = ∑_{i = 1}^d w^{(1)}_{ij} x^{(0)} + b^{(l)}_j$. Chú ý, $b^{(l)}_j$ chỉ xuất hiện một lần duy nhất sau khi tính tổng.\n",
    "\n",
    "Rồi sau đó $a^{(1)} = f(z^{(1)})$ ở layer đầu tiên.\n",
    "Lấy $a^{(1)}$ làm input, ta tính tương tự $z^{(2)}, a^{(2)}$ ở layer tiếp theo.\n",
    "Lặp lại cho đến layer cuối cùng thì được predicted output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quá trình lan truyền xuôi diễn ra liên tục cho đến khi nào kết quả cuối cùng xuất hiện ở output layer. Kết quả này sẽ được so sánh với giá trị thực tế để xác định sai sót, từ đó xây dựng và tối ưu hóa loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lan truyền ngược - Backpropagation\n",
    "\n",
    "Sau khi lan truyền xuôi xong và nhận được output cũng như tính toán được sai số (thường là thông qua một loss function như MSE hoặc Cross-Entropy), lan truyền ngược sẽ bắt đầu. Quá trình này sử dụng quy tắc chuỗi trong giải tích để tính gradient của loss  theo $W$ và $b$ từ layer cuối cùng đến layer đầu tiên. \n",
    "\n",
    "Layer cuối cùng được tính toán trước vì nó gần gũi hơn với predicted outputs và loss function. Việc tính toán gradient của các layer trước được thực hiện dựa trên một quy tắc quen thuộc có tên là chain rule, tức đạo hàm của hàm hợp.\n",
    "\n",
    "*Chú ý, phần này rất nặng về toán*\n",
    "\n",
    "**Nhắc lại**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Đạo hàm của hàm hợp\n",
    "\n",
    "$$ f(x) = A(B(C(x))), \\quad → \\quad f'(x) = \\frac{∂A}{∂B} \\frac{∂B}{∂C} \\frac{∂C}{∂x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hàm hợp với quy tắc chuỗi**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gọi $J(W,b)$ là loss function của bài toán. \n",
    "\n",
    "Tại một cặp node training ở vị trí thứ $i$,$j$ sao cho node thứ $j$ nằm ở layer cuối cùng $(l)$ và node thứ $i$ nằm ở layer trước đó $(l-1)$, ta cần tính gradient của  $J$ đối với $w_{ij}^{(l)}$ và $b_j^{(l)}$ :\n",
    "\n",
    "- Đạo hàm theo $w_{ij}^{(l)}$: $$\\frac{∂J}{∂w_{ij^{(l)}}}= \\frac{∂J}{∂z_j^{(l)}}. \\frac{∂z_j^{(l)}}{∂w_{ij}^{(l)}} \\quad (1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Đạo hàm theo $b_j$: $$\\frac{∂J}{∂b_j^{(l)}}= \\frac{∂J}{∂z_j^{(l)}}. \\frac{∂z_j^{(l)}}{∂b_j^{(l)}} \\quad (2)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vì $z_j^{(l)} = w_{ij}^{(l)}a_i^{(l-1)} + b_j^{(l)} \\quad$ nên $ \\quad \\frac{∂z_j^{(l)}}{∂w_{ij}^{(l)}} = a_i^{(l)} \\quad$ và $ \\quad \\frac{∂z_j^{(l)}}{∂b_j^{(l)}} = 1$. \n",
    "\n",
    "\n",
    "$$\\frac{∂J}{∂w_{ij}^{(l)}}=  \\frac{∂J}{∂z_j^{(l)}}a_i^{(l-1)} \\quad (1)$$\n",
    "\n",
    "Khi đó, 1 và 2 trở thành:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{∂J}{∂b_j^{(l)}}= \\frac{∂J}{∂z_j^{(l)}} \\quad (2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Câu hỏi đặt ra là $\\frac{∂J}{∂z_j^{(l)}}$ tính như thế nào ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mặt khác, vì $J$ là giá trị mất mát nên nó phụ thuộc vào output cuối cùng, khi đó $J$ gián tiếp phụ thuộc vào tất cả các $z^{(l)}_j$ qua lan truyền xuôi.\n",
    "\n",
    "Để tính $\\frac{\\partial J}{\\partial z^{(l)}_j}$, ta áp dụng Quy tắc Dây chuyền:\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial z^{(l)}_j} = \\sum_{k=1}^{d} \\frac{\\partial J}{\\partial z^{(l+1)}_k} \\cdot \\frac{\\partial z^{(l+1)}_k}{\\partial z^{(l)}_j}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lý do chúng ta cần tính tổng với các $z^{(l+1)}_k$ ở layer kế tiếp (l+1) khi tính $\\frac{\\partial J}{\\partial z^{(l)}_j}$ là vì trong mạng neural, mỗi layer được tính dựa trên layer trước đó.\n",
    "\n",
    "Cụ thể, tại layer l, mỗi $z^{(l)}_j$ được tính từ các $a^{(l-1)}_i$ ở layer trước (l-1) qua các weights và biases.\n",
    "\n",
    "Sau đó, các $z^{(l+1)}_k$ tại layer kế tiếp (l+1) lại được tính từ các $a^{(l)}_j = f(z^{(l)}_j)$.\n",
    "\n",
    "Như vậy, các $z^{(l+1)}_k$ phụ thuộc trực tiếp vào các $z^{(l)}_j$, hay nói cách khác, thay đổi $z^{(l)}_j$ sẽ làm thay đổi các $z^{(l+1)}_k$.\n",
    "\n",
    "Để tính $\\frac{\\partial J}{\\partial z^{(l)}_j}$, chúng ta cần xem xét các thay đổi của hàm mất mát $J$ theo các $z^{(l+1)}_k$ đầu tiên ($\\frac{\\partial J}{\\partial z^{(l+1)}_k}$), sau đó tính tổng tích trọng để lan truyền ngược về layer l qua các $\\frac{\\partial z^{(l+1)}_k}{\\partial z^{(l)}_j}$.\n",
    "\n",
    "Vì vậy, công thức đạo hàm cần sử dụng chỉ số (l+1) để lấy thông tin từ layer kế tiếp, rồi mới lan truyền ngược về layer hiện tại l.\n",
    "\n",
    "f′(z(l)j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây là một câu hỏi rất hay, cảm ơn bạn đã chỉ ra điểm này.\n",
    "\n",
    "Lý do chúng ta có nhân với $f'(z^{(l)}_j)$ trong công thức ban đầu là do áp dụng quy tắc dây chuyền cho phép tính đạo hàm của hàm hợp.\n",
    "\n",
    "Trong mạng neural, tại mỗi layer l, giá trị activation $a^{(l)}_j$ được tính từ $z^{(l)}_j$ qua activation function $f$ như sau:\n",
    "\n",
    "$a^{(l)}_j = f(z^{(l)}_j)$\n",
    "\n",
    "Hàm mất mát $J$ phụ thuộc gián tiếp vào các $z^{(l)}_j$ thông qua các $a^{(l)}_j$.\n",
    "\n",
    "Do đó, khi tính $\\frac{\\partial J}{\\partial z^{(l)}_j}$, chúng ta cần áp dụng quy tắc dây chuyền như sau:\n",
    "\n",
    "$\\frac{\\partial J}{\\partial z^{(l)}_j} = \\frac{\\partial J}{\\partial a^{(l)}_j} \\cdot \\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j}$\n",
    "\n",
    "Trong đó, $\\frac{\\partial a^{(l)}_j}{\\partial z^{(l)}_j}$ chính là đạo hàm của activation function $f$ tại điểm $z^{(l)}_j$, được ký hiệu là $f'(z^{(l)}_j)$.\n",
    "\n",
    "Vì vậy, khi tính $\\frac{\\partial J}{\\partial z^{(l)}_j}$, chúng ta cần tính $\\frac{\\partial J}{\\partial a^{(l)}_j}$ trước, sau đó nhân với $f'(z^{(l)}_j)$ để có được đầy đủ đạo hàm theo quy tắc dây chuyền.\n",
    "\n",
    "Tóm lại, phần $f'(z^{(l)}_j)$ trong công thức ban đầu đóng vai trò quan trọng để áp dụng đúng quy tắc dây chuyền khi tính đạo hàm qua các activation function tại mỗi layer trong quá trình backpropagation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
