{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression: learning by gradients descent\n",
    "\n",
    "If you used to learn math in high-school, you would be familiar this plot:\n",
    "\n",
    "![](https://github.com/tiepvupsu/tiepvupsu.github.io/blob/master/assets/GD/gradient_descent.png?raw=true)\n",
    "\n",
    "\n",
    "This image shows some imformation like:\n",
    "\n",
    "* `x` - one point in the x-axis\n",
    "* `f(x)` - a particular function\n",
    "* `f'(x)` - the gradient (also called derivative of function).\n",
    "* the plot represents the function\n",
    "  \n",
    "The green point `x*` you see in the plot is `local minimum` , means that value of function equal 2 if only when `x = 1`. In some cases, we will have `local maximum` if the plot's concavity have direction going down. \n",
    "\n",
    "_remind:_\n",
    "  + _the local minimum `x` is a point at which we possess the gradient `f'(x) = 0`. Other points on the right of it, the gradient will be `positive`. In contrast, Other points on the left will make gradient `negative`_\n",
    "  \n",
    "\n",
    "  + _The slope of a tangent at an any point will equal to gradient at that point._\n",
    "\n",
    "In the plot, the red lines which are tangents at corresponding points. This means that the function has gradient at every point. This is fundamental concept of mathematics.\n",
    "\n",
    "So, let asume that\n",
    "\n",
    ". \n",
    "\n",
    "\n",
    "Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning models by minimizing errors between actual and expected results. \n",
    "\n",
    "Further, gradient descent is also used to train Neural Networks.\n",
    "\n",
    "learning rate \n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x, w):\n",
    "    return w[0] + w[1] * x\n",
    "\n",
    "def loss(x, y, w):\n",
    "    d = 0\n",
    "    for i in range(len(x)):\n",
    "        d += (y[i] - (w[0] + w[1] * x[i]))**2\n",
    "    return d / (2 * len(x))\n",
    "\n",
    "def derivative(x, y, w):\n",
    "    d0 = 0\n",
    "    d1 = 0\n",
    "    for i in range(len(x)):\n",
    "        d1 += x[i] * (f(x[i], w) - y[i])\n",
    "        d0 += f(x[i], w) - y[i]\n",
    "    return d0 / len(x), d1 / len(x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(x, w):\n",
    "    return w[0] + w[1] * x\n",
    "\n",
    "def loss(x, y, w):\n",
    "    d = 0\n",
    "    for i in range(len(x)):\n",
    "        d += (y[i] - (w[0] + w[1] * x[i]))**2\n",
    "    return d / (2 * len(x))\n",
    "\n",
    "def derivative(x, y, w):\n",
    "    d0 = 0\n",
    "    d1 = 0\n",
    "    for i in range(len(x)):\n",
    "        d1 += x[i] * (f(x[i], w) - y[i])\n",
    "        d0 += f(x[i], w) - y[i]\n",
    "    return d0 / len(x), d1 / len(x)\n",
    "\n",
    "# Assuming x and y are defined as numpy arrays of the same length\n",
    "x = np.linspace(start=1, stop=10, num=50)\n",
    "y = 2 * x + np.random.normal(0, 1, 50)  # Example linear data with some noise\n",
    "\n",
    "epoch = 10\n",
    "learning_rate = 0.01\n",
    "w = [1, 1]  # Initial guess for the model parameters\n",
    "los_old = float('inf')  # Initialize as infinity\n",
    "Lương Thị Ngọc Khánh\n",
    "2:42 PM\n",
    "for i in range(epoch):\n",
    "    # Plot the data points\n",
    "    plt.plot(x, y, 'ro')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "\n",
    "    # Plot the current model\n",
    "    x0 = np.linspace(start=1, stop=10, num=50)\n",
    "    y0 = w[0] + w[1] * x0\n",
    "    plt.plot(x0, y0)\n",
    "    plt.show()\n",
    "\n",
    "    # Update parameters\n",
    "    los = loss(x, y, w)\n",
    "    print(\"epoch_\", i, ':')\n",
    "    print(los, \" : \", los_old)\n",
    "\n",
    "    # If loss has not decreased significantly, stop the training\n",
    "    if los > (los_old - 0.0001) and i > 0:\n",
    "        break\n",
    "los_old = los\n",
    "\n",
    "    # Calculate the gradient and update the weights\n",
    "    a, b = derivative(x, y, w)\n",
    "    w[0] = w[0] - a * learning_rate\n",
    "    w[1] = w[1] - b * learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "[machinelearningcoban.com/](https://machinelearningcoban.com/2017/01/12/gradientdescent/)\n",
    "\n",
    "[javatpoint](https://www.javatpoint.com/gradient-descent-in-machine-learning)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
