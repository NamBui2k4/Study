{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nguồn: [machinelearningcoban](https://machinelearningcoban.com/2017/01/27/logisticregression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Thật là kỳ lạ khi lại có một phương pháp mà chỉ cần nhìn cái tên thôi cũng biết là nó dành cho bài toán regression rồi. Nhưng không, chính xác thì nó lại áp dụng cho classification cơ. Bạn không nghe nhầm đâu.\n",
    "\n",
    "## Nhắc lại\n",
    "\n",
    "Trong chương [Linear regression](../regression/LR.ipynb), chúng ta đã nắm rõ được mối quan hệ giữa dữ liệu đầu vào và các kết quả đầu ra thông qua hàm tuyến tính: $$ z = w^Tx =  𝑊_1 𝑥_1 + 𝑊_2 𝑥_2 + ... +𝑊_n 𝑥_n + 𝑏$$\n",
    "\n",
    "Trong đó:\n",
    "- $w^T = ( 𝑊_1,  𝑊_2,..., 𝑊_n)$ là vector tham số cần tìm \n",
    "- $x = (x_1, x_2,...,x_n)$ là vector dữ liệu đầu vào\n",
    "- z chính là đầu ra của mô hình mà chúng ta dự đoán\n",
    "\n",
    "Và xin nhắc lại, yêu cầu của bài toán là đi tìm nghiệm $w^T$ để từ đó xây dựng công thức tổng quát cho z. Hay nói cách khác là từ dữ liệu đầu vào và đầu ra sẵn có, chúng ta huấn luyên mô hình z để nó học và ứng dụng cho những dự đoán khác trong tương lai. \n",
    "\n",
    "Nếu đọc qua chương [Perceptron Learning Algorithm (PLA)](/PLA.ipynb), ta sẽ thấy rằng các thuật toán đều biến đổi z thành một kết quả mới thông qua một hàm gọi là activation function. Hàm đó có thể được viết tạm là $f(z)$.\n",
    "\n",
    "\n",
    "- Đối với Linear Regression: $$f(z)= z = w^Tx$$\n",
    "- Đối với PLA: $$f(z)=sign(w^Tx)$$\n",
    "\n",
    "Còn tại sao phải có actication function thì hãy kham khảo một số nguồn sau: [viblo](https://viblo.asia/p/mot-so-ham-kich-hoat-trong-cac-mo-hinh-deep-learning-tai-sao-chung-lai-quan-trong-den-vay-part-1-ham-sigmoid-bWrZn4Rv5xw), [aicurious.io](https://aicurious.io/blog/2019-09-23-cac-ham-kich-hoat-activation-function-trong-neural-networks), [ITzone](https://www.bing.com/search?pglt=41&q=activation+function+l%C3%A0+g%C3%AC&cvid=cf7118ccfc094fd9be34751b8df1d2b5&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQABhAMgYIAhAAGEAyBggDEAAYQDIGCAQQABhAMgYIBRAAGEAyBggGEAAYQDIGCAcQABhAMgYICBAAGEDSAQg3MjM2ajBqMagCALACAA&FORM=ANNTA1&ucpdpc=UCPD&adppc=EDGEXST&PC=ASTS). Tôi nghĩ chúng có thể giải quyết khúc mắc của các bạn (đó là nếu bạn chịu thắc mắc :v). Là người viết bài này mà lại đi cóp nhặt của bên khác, tôi cũng từng thắc mắc như vậy và đôi khi cũng nổi điên vì các nhà toán học đã nghĩ gì mà lại làm phức tạp bài toán nữa. Nhưng tôi tin rằng mọi thứ đều có lý do của nó.\n",
    " \n",
    "Trở lại với vấn đề, trong bài này, tôi sẽ giới thiệu mô hình thứ ba với một activation khác, được sử dụng cho các bài toán flexible hơn. Đó là mô hình Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đặt vấn đề\n",
    "\n",
    "Vẫn là bài toán quen thuộc, một nhóm 20 sinh viên dành thời gian trong khoảng từ 0 đến 6 giờ cho việc ôn thi. Thời gian ôn thi này ảnh hưởng đến xác suất sinh viên vượt qua kỳ thi như thế nào?\n",
    "\n",
    "Kết quả thu được như sau:\n",
    "\n",
    "| Hours | Pass | Hours | Pass |\n",
    "| ----- | ---- | ----- | ---- |\n",
    "| .5    | 0    | 2.75  | 1    |\n",
    "| .75   | 0    | 3     | 0    |\n",
    "| 1     | 0    | 3.25  | 1    |\n",
    "| 1.25  | 0    | 3.5   | 0    |\n",
    "| 1.5   | 0    | 4     | 1    |\n",
    "| 1.75  | 0    | 4.25  | 1    |\n",
    "| 1.75  | 1    | 4.5   | 1    |\n",
    "| 2     | 0    | 4.75  | 1    |\n",
    "| 2.25  | 1    | 5     | 1    |\n",
    "| 2.5   | 0    | 5.5   | 1    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chúng ta biểu diễn các dữ liệu này trên đồ thị để thấy rõ hơn\n",
    "\n",
    "*Chú ý rằng các điểm màu đỏ và xanh được vẽ ở hai tung độ khác nhau để tiện cho việc minh họa. Các điểm này mô tả cho cả dữ liệu đầu vào x và đầu ra y. Trong đó, đầu vào x nằm trên trục hours studying còn đầu ra y nằm trên trục fail/pass*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://machinelearningcoban.com/assets/LogisticRegression/ex1.png\" style=\"position: relative; margin-top: 20px; heigh:650px;width:650px; margin-bottom:20px; left:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Từ đồ thị, ta có thể thấy rất nhiều người thi đỗ khi số giờ học gia tăng từ 4 trở lên, ngược lại thì số người thi trượt tăng lên khi số giờ học chỉ vỏn vẹn từ 0.5-2 giờ.\n",
    "\n",
    "Mặc dù có một chút bất công ngoại lệ khi học 3.5 giờ mà vẫn trượt, còn học 1.75 giờ thì lại đỗ, thì nhìn chung, học càng nhiều thì khả năng đỗ càng cao. \n",
    "\n",
    "Như vậy, đầu ra có thể được thể hiện dưới dạng xác suất (probability). Ví dụ: xác suất thi đỗ nếu biết thời gian ôn thi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhận thấy rằng cả linear regression và PLA đều không phù hợp với bài toán này với lý do:\n",
    "- Giá trị đầu ra bị giới hạn trong khoảng (0,1), trong khi đường tuyến tính có thể vượt qua khỏi phạm vi đó\n",
    "- PLA không thể áp dụng được cho bài toán này vì không thể nói một người học bao nhiêu giờ thì 100% trượt hay đỗ. Và thực tế là dữ liệu này cũng không linearly separable (điệu kiện để PLA có thể làm việc)\n",
    "\n",
    "Chúng ta cần một mô hình flexible hơn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mô hình Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mô hình mới này của chúng ta có tên là logistic regression. Mô hình này giống với linear regression ở khía cạnh đầu ra là số thực, và giống với PLA ở việc đầu ra bị chặn (trong đoạn [0,1]). Mặc dù trong tên có chứa từ regression, logistic regression thường được sử dụng nhiều hơn cho các bài toán classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đầu ra dự đoán của logistic regression cũng được biến đổi bằng một activation function:\n",
    "$$f(z)=θ(w^Tx)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vậy thì câu hỏi đặt ra là: θ này có dạng cụ thể như thế nào?\n",
    "\n",
    "Chúng ta xét đồ thị của một số hàm sau:\n",
    "\n",
    "\n",
    "<img src=\"https://machinelearningcoban.com/assets/LogisticRegression/activation.png\" style=\"position: relative; margin-top: 20px; heigh:650px;width:650px; margin-bottom:20px; left:200px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Như đã nói ở trên, đường màu vàng biểu diễn linear regression và nó không phù hợp cho bài toán này.\n",
    "\n",
    "Đường màu đỏ (chỉ khác với activation function của PLA ở chỗ hai class là 0 và 1 thay vì -1 và 1) cũng thuộc dạng ngưỡng cứng (hard threshold). PLA không hoạt động trong bài toán này vì dữ liệu đã cho không linearly separable.\n",
    "\n",
    "Các đường màu xanh lam và xanh lục phù hợp với bài toán của chúng ta hơn. Chúng có một vài tính chất quan trọng sau:\n",
    "\n",
    "- Là hàm số liên tục nhận giá trị thực, bị chặn trong khoảng (0,1).\n",
    "- Nếu coi điểm có tung độ là 1/2 làm điểm phân chia thì các điểm càng xa điểm này về phía bên trái có giá trị càng gần 0. Ngược lại, các điểm càng xa điểm này về phía phải có giá trị càng gần 1. Điều này khớp với nhận xét rằng học càng nhiều thì xác suất đỗ càng cao và ngược lại.\n",
    "- Mượt (smooth) nên có đạo hàm mọi nơi, có thể được lợi trong việc tối ưu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Để có được các đường cong như vậy, một hàm phổ biến mà logistic function áp dụng chính là hàm sigmoid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "Công thức:\n",
    "\n",
    "  $$f(z)=\\frac{1}{1+𝑒^{−z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm này được sử dụng nhiều nhất, vì nó bị chặn trong khoảng $(0,1)$. Thật vậy:\n",
    "\n",
    "$$\\lim_{z → −∞} f(z) = 0 \\quad  \\quad \\lim_{z→+∞} f(z) =1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đặc biệt hơn nữa, nếu ta lấy đạo hàm thì kết quả sẽ vô cùng đẹp mắt:\n",
    "\n",
    "$$\n",
    "f'(z)\n",
    "\n",
    "  = \\frac{e^{−z}}{(1+e^{−z})^2}\n",
    "  \\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "  = \\frac{1}{1+e−z}\\frac{e^{−z}}{1+e^{−z}}\n",
    "  \\\\\n",
    "$$\n",
    "$$  \n",
    "  \\\\\n",
    "  = f(z)(1−f(z))\n",
    "  \\\\\n",
    "  \\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Công thức đạo hàm đơn giản thế này giúp hàm số này được sử dụng rộng rãi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kham khảo thêm: sigmoid function có rất nhiều biến thể, chẳng hạn như\n",
    "\n",
    "- **Hàm logistic hyperbolic**: $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$\n",
    "- **Hàm Arctanh (Arctan):**: $\\text{Arctanh}(x) = \\frac{1}{2}\\log\\bigg(\\frac{1+x}{1-x}\\bigg)$\n",
    "​\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xây dựng hàm mất mát"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "giả sử rằng xác suất để một điểm dữ liệu $x_i$ rơi vào class 1 là $f(z)$ và rơi vào class 0 là $1−f(z)$. Với mô hình như vậy, với các điểm dữ liệu training (đã biết đầu ra y), ta có thể viết như sau:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(y_i=1  |x_i;w)= f(z) \\quad (1)$$\n",
    "\n",
    "$$P(y_i=0 |x_i;w)=1 - f(z) \\quad (2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Đây chính là xác suất có điều kiện. Mục đích của chúng ta là tìm các hệ số $w$ sao cho $f(z) = θ(w^Tx)$ càng gần với 1 càng tốt với các điểm dữ liệu thuộc class 1 và càng gần với 0 càng tốt với những điểm thuộc class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ta có thể viết gộp công thức trên thành một công thức tổng quát: $$P(y_i|x_i;w)=z_i^{yi}(1−z_i)^{1−y_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nhưng đó mới chỉ là xác suất cho một điểm dữ liệu. Xét toàn bộ training set với $X=[x_1,x_2,…,x_N]∈R^{d×N}$ và $y_=[y_1,y_2,…,y_N]$, ta có:\n",
    "\n",
    "$$P(y|X;w) = ∏_{i=1}^2 z^{y_i}_i(1−z_i)1−y_i$$\n",
    "\n",
    "Bài toán tìm tham số $w$ để mô hình gần với dữ liệu nhất trên đây có tên gọi chung là bài toán maximum likelihood estimation với hàm số phía sau được gọi là likelihood function. Khi làm việc với các bài toán Machine Learning sử dụng các mô hình xác suất thống kê, chúng ta sẽ gặp lại các bài toán thuộc dạng này"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
