{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BÃ i viáº¿t Ä‘Æ°á»£c láº¥y tá»« nguá»“n: [mmlab.uit](https://mmlab.uit.edu.vn/tutorials/ml/gradient-based-model/logistic_regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÃ´ hÃ¬nh Logistic Regression\n",
    "\n",
    "**Tá»•ng quan**\n",
    "\n",
    "Logistic Regression Ä‘Æ°á»£c gá»i lÃ  phÃ¢n lá»›p nhá»‹ phÃ¢n - Binary Classification cá»¥ thá»ƒ mÃ´ hÃ¬nh nÃ y dÃ¹ng Ä‘á»ƒ dá»± Ä‘oÃ¡n output dá»±a vÃ o cÃ¡c giÃ¡ trá»‹ input Ä‘Ã£ cho. Háº§u háº¿t output cá»§a Losgistic Regression chá»‰ cÃ³ 2 giÃ¡ trá»‹ nhÆ°: True/False, Yes/No, 0/1,... Má»™t sá»‘ vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh lÃ  phÃ¢n loáº¡i mail cÃ³ pháº£i spam hay khÃ´ng, phÃ¢n loáº¡i khÃ¡ch hÃ ng cÃ³ pháº£i tiá»m nÄƒng hay khÃ´ng.\n",
    "\n",
    "**VÃ­ dá»¥**\n",
    "\n",
    "Má»™t nhÃ³m 20 sinh viÃªn dÃ nh thá»i gian trong khoáº£ng tá»« 0 Ä‘áº¿n 6 giá» cho viá»‡c Ã´n thi. Thá»i gian Ã´n thi nÃ y áº£nh hÆ°á»Ÿng Ä‘áº¿n xÃ¡c suáº¥t sinh viÃªn vÆ°á»£t qua ká»³ thi nhÆ° tháº¿ nÃ o?\n",
    "\n",
    "| Hours | Pass | Hours | Pass |\n",
    "| ----- | ---- | ----- | ---- |\n",
    "| .5    | 0    | 2.75  | 1    |\n",
    "| .75   | 0    | 3     | 0    |\n",
    "| 1     | 0    | 3.25  | 1    |\n",
    "| 1.25  | 0    | 3.5   | 0    |\n",
    "| 2.25  | 1    | 5     | 1    |\n",
    "| 2.5   | 0    | 5.5   | 1    |\n",
    "\n",
    "Náº¿u Ã¡p dá»¥ng  Linear Regression  vÃ o bÃ i toÃ¡n nÃ y, ta  khÃ³ cÃ³ thá»ƒ Ä‘áº¡t káº¿t quáº£ tá»‘i Æ°u (xem hÃ¬nh dÆ°á»›i).\n",
    "\n",
    "![](https://i.imgur.com/LKTj7Uy.png)\n",
    "\n",
    "\n",
    "Do Ä‘Ã³ chÃºng ta cáº§n má»™t mÃ´ hÃ¬nh phÃ¹ há»£p hÆ¡n Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n tÆ°Æ¡ng tá»±: Logistic Regression.\n",
    "\n",
    "## Sá»± giá»‘ng vÃ  khÃ¡c nhau giá»¯a Linear Regression vÃ  Logistic Regression\n",
    "\n",
    "Giá»‘ng nhau:\n",
    "\n",
    "- Linear Regression vÃ  Logistic Regression Ä‘á»u lÃ  nhá»¯ng mÃ´ hÃ¬nh mÃ¡y há»c cÃ³ giÃ¡m sÃ¡t.\n",
    "- Äá»u sá»­ dá»¥ng phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh cho dá»± Ä‘oÃ¡n.\n",
    "\n",
    "KhÃ¡c nhau:\n",
    "\n",
    "| Linear Regression                                          | Logistic Regression                                                      |\n",
    "| ---------------------------------------------------------- | ------------------------------------------------------------------------ |\n",
    "| Sá»­ dá»¥ng cho cÃ¡c bÃ i toÃ¡n há»“i quy                           | Sá»­ dá»¥ng cho cÃ¡c bÃ i toÃ¡n phÃ¢n lá»›p                                        |\n",
    "| Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ cá»§a biáº¿n liÃªn tá»¥c                          | Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ cá»§a biáº¿n rá»i ráº¡c                                         |\n",
    "| Sá»­ dá»¥ng hÃ m Ä‘á»™ lá»—i Mean Square Error                       | Sá»­ dá»¥ng hÃ m Ä‘á»™ lá»—i Cross Entropy                                         |\n",
    "| Má»¥c Ä‘Ã­ch lÃ  tÃ¬m Ä‘Æ°á»ng tháº³ng phÃ¹ há»£p Ä‘á»ƒ dá»± Ä‘oÃ¡n cÃ¡c giÃ¡ trá»‹ | Má»¥c Ä‘Ã­ch lÃ  tÃ¬m ra Ä‘Æ°á»ng cong (Ä‘Æ°á»ng cong sigmoid) Ä‘á»ƒ phÃ¢n biá»‡t cÃ¡c biáº¿n |\n",
    "| Äáº§u ra lÃ  cÃ¡c giÃ¡ trá»‹ liÃªn tá»¥c                             | Äáº§u ra lÃ  cÃ¡c giÃ¡ trá»‹ trong khoáº£ng (0,1)                                 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function\n",
    "\n",
    "NhÃ¬n chung Logistic Regression khÃ¡ giá»‘ng vá»›i Linear Regression Ä‘á»u sá»­ dá»¥ng hÃ m tuyáº¿n tÃ­nh Ä‘á»ƒ biá»ƒu diá»…n dá»¯ liá»‡u\n",
    "\n",
    "HÃ m tuyáº¿n tÃ­nh: $$ y = ğ‘Š_1 ğ‘¥_1 + ğ‘Š_2 ğ‘¥_2 + ... +ğ‘Š_n ğ‘¥_n + ğ‘$$\n",
    "\n",
    "Tuy nhiÃªn Logistic Regression láº¡i sá»­ dá»¥ng thÃªm hÃ m sigmoid Ä‘á»ƒ chuáº©n hoÃ¡ dá»¯ liá»‡u y vá» khoáº£ng (0,1)\n",
    "\n",
    "HÃ m sigmoid:    $$z = ğ‘“(y)=\\frac{1}{1+ğ‘’^{âˆ’y}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TÃ­nh cháº¥t hÃ m sá»‘:\n",
    "\n",
    "- HÃ m liÃªn tá»¥c, cho giÃ¡ trá»‹ trong khoáº£ng (0,1).\n",
    "- CÃ³ Ä‘áº¡o hÃ m trÃªn má»i Ä‘iá»ƒm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex\">\n",
    "    <p style=\"position: relative; top:100px; margin-right:90px\">Äá»“ thá»‹ hÃ m sigmoid</p>\n",
    "    <img src=\"https://i.imgur.com/WCoP88d.png\" style=\"position: relative; margin-top: 20px;heigh:450px;width:450px; margin-bottom:20px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HÃ m máº¥t mÃ¡t - Loss function\n",
    "\n",
    "HÃ m máº¥t mÃ¡t cá»§a bÃ i toÃ¡n sáº½ thay Ä‘á»•i tÃ¹y thuá»™c vÃ o ngá»¯ cáº£nh bÃ i toÃ¡n\n",
    "\n",
    "**1. Khi bÃ i toÃ¡n yÃªu cáº§u phÃ¢n loáº¡i nhá»‹ phÃ¢n (binary classification)**\n",
    "\n",
    "$$Loss = âˆ‘_{(x,y)\\in D} -y \\log(y') - (1-y) \\log(1-y')$$\n",
    "\n",
    "**1. Khi bÃ i toÃ¡n yÃªu cáº§u phÃ¢n nhiá»u lá»›p (multi-class classification)**\n",
    "\n",
    "$$ Loss = \\frac{1}{n}âˆ‘yâ€‹ â‹…log(â€‹y') $$\n",
    "\n",
    "Trong Ä‘Ã³: \n",
    "- $y$ lÃ  cÃ¡c nhÃ£n Ä‘Ã£ biáº¿t trong táº­p dá»¯ liá»‡u\n",
    "- $y'$ lÃ  nhÃ£n mÃ  chÃºng ta dá»± Ä‘oÃ¡n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triá»ƒn khai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Import**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df =pd.read_csv('Bank_Personal_Loan_Modelling.csv')\n",
    "\n",
    "X = np.array([0.5, 0.75, 1, 1.25, 1.5, 1.75, 1.75, 2, 2.25, 2.5, 2.75, 3, 3.25, 3.5, 4, 4.25, 4.5, 4.75, 5, 5.5])\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**3. Split data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Initialize sigmoid**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Initialize Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    # Clip predictions to avoid log(0) error\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Initialize parameter**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1 #learning late\n",
    "W = np.random.uniform(0,1) # colom 1\n",
    "b = 0.1\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Run model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    z = np.dot(X_train, W) + b\n",
    "    y_pred = sigmoid(z)\n",
    "    # print(cross_entropy_loss(predict(X_train, W, b), y_train))\n",
    "    gradient_W = np.dot((y_pred-y_train).T, X_train)/X_train.shape[0]\n",
    "    gradient_b = np.mean(y_pred-y_train)\n",
    "    W = W - lr * gradient_W\n",
    "    b = b - lr* gradient_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. Predict**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W, b):\n",
    "    z = np.dot(X, W) + b\n",
    "    z = sigmoid(z)\n",
    "    y_pred = [int(i > 0.5) for i in z]\n",
    "    return np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: 0.8\n",
      "Train: 0.75\n",
      "data: 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "print('Test:', f1_score(predict(X_test, W, b), y_test))\n",
    "print('Train:', f1_score(predict(X_train, W, b), y_train))\n",
    "print('data:', f1_score(predict(X, W, b), y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
